{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_index import SimpleDirectoryReader, GPTSimpleVectorIndex, LLMPredictor, PromptHelper\n",
    "from langchain import OpenAI\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVectorIndex(path):\n",
    "    max_input = 4096\n",
    "    tokens = 256\n",
    "    chunk_size = 600\n",
    "    max_chunk_overlap = 20\n",
    "\n",
    "    prompt_helper = PromptHelper(max_input, tokens, max_chunk_overlap, chunk_size_limit=chunk_size)\n",
    "\n",
    "    #Define LLM\n",
    "    llMPredictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-ada-001\", max_tokens=tokens))\n",
    "\n",
    "    #load data\n",
    "    docs = SimpleDirectoryReader(path).load_data()\n",
    "\n",
    "    #create vector Index\n",
    "    vectorIndex = GPTSimpleVectorIndex(documents=docs, llm_predictor=llMPredictor, prompt_helper=prompt_helper)\n",
    "    vectorIndex.save_to_disk('vectorIndex.json')\n",
    "    \n",
    "    return vectorIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 5392 tokens\n"
     ]
    }
   ],
   "source": [
    "vectorIndex = createVectorIndex('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answerMe(vectorIndex):\n",
    "    vIndex = GPTSimpleVectorIndex.load_from_disk('vectorIndex.json')\n",
    "    while True:\n",
    "        prompt = input('What is your questions:')\n",
    "        print(f\"Question: {prompt} \\n\")\n",
    "        response = vIndex.query(prompt, response_mode=\"compact\")\n",
    "        print(f\"Response: {response} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what the docs is about? \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 733 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 6 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "Dieser Dokument beschreibt Symptome und Behandlungsmöglichkeiten von Diabetes Typ 2. Es erklärt, wie man die Krankheit verstehen und behandeln kann, einschließlich der Unterstützung durch Ärzte und andere Fachleute, sowie die Teilnahme an einem Disease-Management-Programm. \n",
      "\n",
      "Question:  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:openai:error_code=None error_message=\"[''] is not valid under any of the given schemas - 'input'\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "INFO:openai:error_code=None error_message=\"[''] is not valid under any of the given schemas - 'input'\" error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m answerMe(\u001b[39m'\u001b[39;49m\u001b[39mvectorIndex.json\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m, in \u001b[0;36manswerMe\u001b[1;34m(vectorIndex)\u001b[0m\n\u001b[0;32m      4\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mWhat is your questions:\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mQuestion: \u001b[39m\u001b[39m{\u001b[39;00mprompt\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m response \u001b[39m=\u001b[39m vIndex\u001b[39m.\u001b[39;49mquery(prompt, response_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcompact\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mResponse: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gpt_index\\indices\\base.py:395\u001b[0m, in \u001b[0;36mBaseGPTIndex.query\u001b[1;34m(self, query_str, mode, query_transform, use_async, **query_kwargs)\u001b[0m\n\u001b[0;32m    379\u001b[0m query_config \u001b[39m=\u001b[39m QueryConfig(\n\u001b[0;32m    380\u001b[0m     index_struct_type\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_struct\u001b[39m.\u001b[39mget_type(),\n\u001b[0;32m    381\u001b[0m     query_mode\u001b[39m=\u001b[39mmode_enum,\n\u001b[0;32m    382\u001b[0m     query_kwargs\u001b[39m=\u001b[39mquery_kwargs,\n\u001b[0;32m    383\u001b[0m )\n\u001b[0;32m    384\u001b[0m query_runner \u001b[39m=\u001b[39m QueryRunner(\n\u001b[0;32m    385\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm_predictor,\n\u001b[0;32m    386\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prompt_helper,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m     use_async\u001b[39m=\u001b[39muse_async,\n\u001b[0;32m    394\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[39mreturn\u001b[39;00m query_runner\u001b[39m.\u001b[39;49mquery(query_str, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_index_struct)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gpt_index\\indices\\query\\query_runner.py:127\u001b[0m, in \u001b[0;36mQueryRunner.query\u001b[1;34m(self, query_str_or_bundle, index_struct)\u001b[0m\n\u001b[0;32m    125\u001b[0m     query_bundle \u001b[39m=\u001b[39m query_str_or_bundle\n\u001b[0;32m    126\u001b[0m query_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_query_obj(index_struct)\n\u001b[1;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m query_obj\u001b[39m.\u001b[39;49mquery(query_bundle)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gpt_index\\token_counter\\token_counter.py:55\u001b[0m, in \u001b[0;36mllm_token_counter.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[1;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     52\u001b[0m start_token_ct \u001b[39m=\u001b[39m llm_predictor\u001b[39m.\u001b[39mtotal_tokens_used\n\u001b[0;32m     53\u001b[0m start_embed_token_ct \u001b[39m=\u001b[39m embed_model\u001b[39m.\u001b[39mtotal_tokens_used\n\u001b[1;32m---> 55\u001b[0m f_return_val \u001b[39m=\u001b[39m f(_self, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     57\u001b[0m net_tokens \u001b[39m=\u001b[39m llm_predictor\u001b[39m.\u001b[39mtotal_tokens_used \u001b[39m-\u001b[39m start_token_ct\n\u001b[0;32m     58\u001b[0m llm_predictor\u001b[39m.\u001b[39mlast_token_usage \u001b[39m=\u001b[39m net_tokens\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gpt_index\\indices\\query\\base.py:342\u001b[0m, in \u001b[0;36mBaseGPTIndexQuery.query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39m@llm_token_counter\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mquery\u001b[39m(\u001b[39mself\u001b[39m, query_bundle: QueryBundle) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RESPONSE_TYPE:\n\u001b[0;32m    341\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Answer a query.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(query_bundle)\n\u001b[0;32m    343\u001b[0m     \u001b[39m# if include_summary is True, then include summary text in answer\u001b[39;00m\n\u001b[0;32m    344\u001b[0m     \u001b[39m# summary text is set through `set_text` on the underlying index.\u001b[39;00m\n\u001b[0;32m    345\u001b[0m     \u001b[39m# TODO: refactor response builder to be in the __init__\u001b[39;00m\n\u001b[0;32m    346\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_mode \u001b[39m!=\u001b[39m ResponseMode\u001b[39m.\u001b[39mNO_TEXT \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_include_summary:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gpt_index\\indices\\query\\base.py:312\u001b[0m, in \u001b[0;36mBaseGPTIndexQuery._query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Answer a query.\"\"\"\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[39m# TODO: remove _query and just use query\u001b[39;00m\n\u001b[1;32m--> 312\u001b[0m tuples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_nodes_and_similarities_for_response(query_bundle)\n\u001b[0;32m    314\u001b[0m \u001b[39m# prepare response builder\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_response_builder(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_builder, query_bundle, tuples)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gpt_index\\indices\\query\\base.py:242\u001b[0m, in \u001b[0;36mBaseGPTIndexQuery.get_nodes_and_similarities_for_response\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Get list of tuples of node and similarity for response.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \n\u001b[0;32m    237\u001b[0m \u001b[39mFirst part of the tuple is the node.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39mSecond part of tuple is the distance from query to the node.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39mIf not applicable, it's None.\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    241\u001b[0m similarity_tracker \u001b[39m=\u001b[39m SimilarityTracker()\n\u001b[1;32m--> 242\u001b[0m nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_nodes_for_response(\n\u001b[0;32m    243\u001b[0m     query_bundle, similarity_tracker\u001b[39m=\u001b[39;49msimilarity_tracker\n\u001b[0;32m    244\u001b[0m )\n\u001b[0;32m    245\u001b[0m nodes \u001b[39m=\u001b[39m [\n\u001b[0;32m    246\u001b[0m     node \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m nodes \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_use_node(node, similarity_tracker)\n\u001b[0;32m    247\u001b[0m ]\n\u001b[0;32m    249\u001b[0m \u001b[39m# TODO: create a `display` method to allow subclasses to print the Node\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gpt_index\\indices\\query\\vector_store\\base.py:45\u001b[0m, in \u001b[0;36mGPTVectorStoreIndexQuery._get_nodes_for_response\u001b[1;34m(self, query_bundle, similarity_tracker)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_nodes_for_response\u001b[39m(\n\u001b[0;32m     41\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     42\u001b[0m     query_bundle: QueryBundle,\n\u001b[0;32m     43\u001b[0m     similarity_tracker: Optional[SimilarityTracker] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Node]:\n\u001b[1;32m---> 45\u001b[0m     query_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embed_model\u001b[39m.\u001b[39;49mget_agg_embedding_from_queries(\n\u001b[0;32m     46\u001b[0m         query_bundle\u001b[39m.\u001b[39;49membedding_strs\n\u001b[0;32m     47\u001b[0m     )\n\u001b[0;32m     49\u001b[0m     query_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vector_store\u001b[39m.\u001b[39mquery(\n\u001b[0;32m     50\u001b[0m         query_embedding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_similarity_top_k, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_doc_ids\n\u001b[0;32m     51\u001b[0m     )\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m query_result\u001b[39m.\u001b[39mnodes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gpt_index\\embeddings\\base.py:79\u001b[0m, in \u001b[0;36mBaseEmbedding.get_agg_embedding_from_queries\u001b[1;34m(self, queries, agg_fn)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_agg_embedding_from_queries\u001b[39m(\n\u001b[0;32m     74\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     75\u001b[0m     queries: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m     76\u001b[0m     agg_fn: Optional[Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, List[\u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     77\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mfloat\u001b[39m]:\n\u001b[0;32m     78\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get aggregated embedding from multiple queries.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m     query_embeddings \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_query_embedding(query) \u001b[39mfor\u001b[39;00m query \u001b[39min\u001b[39;00m queries]\n\u001b[0;32m     80\u001b[0m     agg_fn \u001b[39m=\u001b[39m agg_fn \u001b[39mor\u001b[39;00m mean_agg\n\u001b[0;32m     81\u001b[0m     \u001b[39mreturn\u001b[39;00m agg_fn(query_embeddings)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gpt_index\\embeddings\\base.py:79\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_agg_embedding_from_queries\u001b[39m(\n\u001b[0;32m     74\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     75\u001b[0m     queries: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m     76\u001b[0m     agg_fn: Optional[Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, List[\u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     77\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mfloat\u001b[39m]:\n\u001b[0;32m     78\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get aggregated embedding from multiple queries.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m     query_embeddings \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_query_embedding(query) \u001b[39mfor\u001b[39;00m query \u001b[39min\u001b[39;00m queries]\n\u001b[0;32m     80\u001b[0m     agg_fn \u001b[39m=\u001b[39m agg_fn \u001b[39mor\u001b[39;00m mean_agg\n\u001b[0;32m     81\u001b[0m     \u001b[39mreturn\u001b[39;00m agg_fn(query_embeddings)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gpt_index\\embeddings\\base.py:68\u001b[0m, in \u001b[0;36mBaseEmbedding.get_query_embedding\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_query_embedding\u001b[39m(\u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mfloat\u001b[39m]:\n\u001b[0;32m     67\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get query embedding.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     query_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_query_embedding(query)\n\u001b[0;32m     69\u001b[0m     query_tokens_count \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer(query))\n\u001b[0;32m     70\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_total_tokens_used \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m query_tokens_count\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gpt_index\\embeddings\\openai.py:222\u001b[0m, in \u001b[0;36mOpenAIEmbedding._get_query_embedding\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid mode, model combination: \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    221\u001b[0m     engine \u001b[39m=\u001b[39m _QUERY_MODE_MODEL_DICT[key]\n\u001b[1;32m--> 222\u001b[0m \u001b[39mreturn\u001b[39;00m get_embedding(query, engine\u001b[39m=\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(f, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tenacity\\__init__.py:389\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoSleep):\n\u001b[0;32m    388\u001b[0m     retry_state\u001b[39m.\u001b[39mprepare_for_next_attempt()\n\u001b[1;32m--> 389\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msleep(do)\n\u001b[0;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m     \u001b[39mreturn\u001b[39;00m do\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tenacity\\nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[1;34m(seconds)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msleep\u001b[39m(seconds: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[39m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     time\u001b[39m.\u001b[39;49msleep(seconds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "answerMe('vectorIndex.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b7d32af1e689683b59f84af5e00ff8511e054604b74fc1830ce1b819250cf76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
